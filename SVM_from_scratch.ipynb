{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cvxpy as cp\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose\n",
    "I'd like to better understand the support vector machine and the formulation and optimization of the dual problem central to it. In the following I make my own SVM classifier with a toy dataset in order to do some of this exploration.\n",
    "\n",
    "### Load up and normalize the data\n",
    "Here there are data for individuals' income, credit limit, credit rating, etc. and a few binary variables.  We can try to use a few of these features to predict one of the binary variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Balance</th>\n",
       "      <th>Caucasian</th>\n",
       "      <th>Asian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.891</td>\n",
       "      <td>3606.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>333.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106.025</td>\n",
       "      <td>6645.0</td>\n",
       "      <td>483.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>903.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104.593</td>\n",
       "      <td>7075.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>580.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>148.924</td>\n",
       "      <td>9504.0</td>\n",
       "      <td>681.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>964.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.882</td>\n",
       "      <td>4897.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>331.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Income   Limit  Rating  Cards   Age  Education  Sex  Student  Married  \\\n",
       "0   14.891  3606.0   283.0    2.0  34.0       11.0  0.0      0.0      1.0   \n",
       "1  106.025  6645.0   483.0    3.0  82.0       15.0  1.0      1.0      1.0   \n",
       "2  104.593  7075.0   514.0    4.0  71.0       11.0  0.0      0.0      0.0   \n",
       "3  148.924  9504.0   681.0    3.0  36.0       11.0  1.0      0.0      0.0   \n",
       "4   55.882  4897.0   357.0    2.0  68.0       16.0  0.0      0.0      1.0   \n",
       "\n",
       "   Balance  Caucasian  Asian  \n",
       "0    333.0        1.0    0.0  \n",
       "1    903.0        0.0    1.0  \n",
       "2    580.0        0.0    1.0  \n",
       "3    964.0        0.0    1.0  \n",
       "4    331.0        1.0    0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_df = pd.read_table('Credit.dat',\n",
    "                         delimiter=' ',\n",
    "                         usecols=np.arange(1,13))\n",
    "credit_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've normalized the columns such that each value is replaced by its difference from the mean, scaled by the spread of the data in that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  standardize all columns\n",
    "credit_df_norm = (credit_df - credit_df.mean())/(credit_df.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Balance</th>\n",
       "      <th>Caucasian</th>\n",
       "      <th>Asian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.860505</td>\n",
       "      <td>-0.489386</td>\n",
       "      <td>-0.464957</td>\n",
       "      <td>-0.698255</td>\n",
       "      <td>-1.256101</td>\n",
       "      <td>-0.783948</td>\n",
       "      <td>-1.034339</td>\n",
       "      <td>-0.332916</td>\n",
       "      <td>0.794400</td>\n",
       "      <td>-0.406768</td>\n",
       "      <td>1.003756</td>\n",
       "      <td>-0.584317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.725276</td>\n",
       "      <td>0.827225</td>\n",
       "      <td>0.827667</td>\n",
       "      <td>0.030993</td>\n",
       "      <td>1.526539</td>\n",
       "      <td>0.495967</td>\n",
       "      <td>0.964384</td>\n",
       "      <td>2.996248</td>\n",
       "      <td>0.794400</td>\n",
       "      <td>0.833013</td>\n",
       "      <td>-0.993768</td>\n",
       "      <td>1.707122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.684646</td>\n",
       "      <td>1.013518</td>\n",
       "      <td>1.028023</td>\n",
       "      <td>0.760241</td>\n",
       "      <td>0.888851</td>\n",
       "      <td>-0.783948</td>\n",
       "      <td>-1.034339</td>\n",
       "      <td>-0.332916</td>\n",
       "      <td>-1.255665</td>\n",
       "      <td>0.130471</td>\n",
       "      <td>-0.993768</td>\n",
       "      <td>1.707122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.942467</td>\n",
       "      <td>2.065853</td>\n",
       "      <td>2.107363</td>\n",
       "      <td>0.030993</td>\n",
       "      <td>-1.140158</td>\n",
       "      <td>-0.783948</td>\n",
       "      <td>0.964384</td>\n",
       "      <td>-0.332916</td>\n",
       "      <td>-1.255665</td>\n",
       "      <td>0.965691</td>\n",
       "      <td>-0.993768</td>\n",
       "      <td>1.707122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.302549</td>\n",
       "      <td>0.069925</td>\n",
       "      <td>0.013314</td>\n",
       "      <td>-0.698255</td>\n",
       "      <td>0.714936</td>\n",
       "      <td>0.815946</td>\n",
       "      <td>-1.034339</td>\n",
       "      <td>-0.332916</td>\n",
       "      <td>0.794400</td>\n",
       "      <td>-0.411118</td>\n",
       "      <td>1.003756</td>\n",
       "      <td>-0.584317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Income     Limit    Rating     Cards       Age  Education       Sex  \\\n",
       "0 -0.860505 -0.489386 -0.464957 -0.698255 -1.256101  -0.783948 -1.034339   \n",
       "1  1.725276  0.827225  0.827667  0.030993  1.526539   0.495967  0.964384   \n",
       "2  1.684646  1.013518  1.028023  0.760241  0.888851  -0.783948 -1.034339   \n",
       "3  2.942467  2.065853  2.107363  0.030993 -1.140158  -0.783948  0.964384   \n",
       "4  0.302549  0.069925  0.013314 -0.698255  0.714936   0.815946 -1.034339   \n",
       "\n",
       "    Student   Married   Balance  Caucasian     Asian  \n",
       "0 -0.332916  0.794400 -0.406768   1.003756 -0.584317  \n",
       "1  2.996248  0.794400  0.833013  -0.993768  1.707122  \n",
       "2 -0.332916 -1.255665  0.130471  -0.993768  1.707122  \n",
       "3 -0.332916 -1.255665  0.965691  -0.993768  1.707122  \n",
       "4 -0.332916  0.794400 -0.411118   1.003756 -0.584317  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_df_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some columns are already either 0 or 1, and I'm going to leave these as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#  revert columns that range from 0 to 1\n",
    "for col in credit_df.iteritems():\n",
    "    if ((col[1].max() - col[1].min()) == 1.0):\n",
    "        credit_df_norm[col[0]] = credit_df[col[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Limit</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Cards</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Student</th>\n",
       "      <th>Married</th>\n",
       "      <th>Balance</th>\n",
       "      <th>Caucasian</th>\n",
       "      <th>Asian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.860505</td>\n",
       "      <td>-0.489386</td>\n",
       "      <td>-0.464957</td>\n",
       "      <td>-0.698255</td>\n",
       "      <td>-1.256101</td>\n",
       "      <td>-0.783948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.406768</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.725276</td>\n",
       "      <td>0.827225</td>\n",
       "      <td>0.827667</td>\n",
       "      <td>0.030993</td>\n",
       "      <td>1.526539</td>\n",
       "      <td>0.495967</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.684646</td>\n",
       "      <td>1.013518</td>\n",
       "      <td>1.028023</td>\n",
       "      <td>0.760241</td>\n",
       "      <td>0.888851</td>\n",
       "      <td>-0.783948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.130471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.942467</td>\n",
       "      <td>2.065853</td>\n",
       "      <td>2.107363</td>\n",
       "      <td>0.030993</td>\n",
       "      <td>-1.140158</td>\n",
       "      <td>-0.783948</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.965691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.302549</td>\n",
       "      <td>0.069925</td>\n",
       "      <td>0.013314</td>\n",
       "      <td>-0.698255</td>\n",
       "      <td>0.714936</td>\n",
       "      <td>0.815946</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.411118</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Income     Limit    Rating     Cards       Age  Education  Sex  Student  \\\n",
       "0 -0.860505 -0.489386 -0.464957 -0.698255 -1.256101  -0.783948  0.0      0.0   \n",
       "1  1.725276  0.827225  0.827667  0.030993  1.526539   0.495967  1.0      1.0   \n",
       "2  1.684646  1.013518  1.028023  0.760241  0.888851  -0.783948  0.0      0.0   \n",
       "3  2.942467  2.065853  2.107363  0.030993 -1.140158  -0.783948  1.0      0.0   \n",
       "4  0.302549  0.069925  0.013314 -0.698255  0.714936   0.815946  0.0      0.0   \n",
       "\n",
       "   Married   Balance  Caucasian  Asian  \n",
       "0      1.0 -0.406768        1.0    0.0  \n",
       "1      1.0  0.833013        0.0    1.0  \n",
       "2      0.0  0.130471        0.0    1.0  \n",
       "3      0.0  0.965691        0.0    1.0  \n",
       "4      1.0 -0.411118        1.0    0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit_df_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make training and target arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since we have 400 data points to work with, I'll use half to train and half to validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target='Married'\n",
    "training_points = 200\n",
    "train_df = credit_df_norm.drop(target, axis=1).loc[:training_points]\n",
    "train_target = credit_df_norm[target].replace(0, -1).loc[:training_points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### definition of the problem\n",
    "\n",
    "given data points $\\vec x_j \\in \\mathbb{R}^{1 \\times N}$ and targets $y_j = \\pm 1$, where $j = 1, \\dots, M$, find the maximum-margin hyperplane that separates the two classes ($y_j = 1$ and $y_j = -1$).  \n",
    "\n",
    "Let $\\vec w$ be the vector normal to the hyperplane.  We want to find $\\vec w$ that satisfies \n",
    "\n",
    "$$ y_j (\\vec w \\cdot \\vec x_j + b) \\geq 1 $$\n",
    "\n",
    "The dual formulation of the above is equivalent to maximizing the following over the multipliers $\\vec \\alpha$:\n",
    "\n",
    "$$ L(\\vec \\alpha) = \\vec y \\cdot \\vec \\alpha  - \\frac 1 2  \\vec \\alpha K \\vec \\alpha^T$$\n",
    "\n",
    "subject to the constraints $\\sum_{j=1}^M \\alpha_j = 0$ and $y_j \\alpha_j \\geq 0$.  The matrix $K$ defines the kernel of the SVM; I've chosen $K_{jk} = k(\\vec x_j, \\vec x_k) = \\vec x_j \\cdot \\vec x_k$.  The parameters of the plane are recovered from $\\vec w = \\vec \\alpha \\cdot \\vec x$ and $b = y_j = \\vec w \\cdot \\alpha_j$ for $j$ such that $\\alpha_j \\neq 0$\n",
    "\n",
    "for reference see eq(1) from:\n",
    "https://arxiv.org/pdf/1307.0471.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the kernel matrix\n",
    "since I've defined the kernel as a symmetric bilinear form, the kernel matrix defining the transformation should also be symmetric.  Further, since $ k (\\vec x_j, \\vec x_k) \\equiv \\vec x_j \\cdot \\vec x_k$ the transformation (and thus the matrix) is positive semidefinite ($x_j^i$ admits entries equal to zero).  This is a well known result for linear kernels.  \n",
    "\n",
    "We can construct the $K$ matrix as follows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_value = np.array(train_df @ train_df.T + np.identity(len(train_target))*1e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the trick here is to add a small constant value along $K$'s diagonal to ensure that small numerical errors that get propagated while calculating eigenvalues don't trigger warnings that our $K$ isn't actually PSD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.04153742, -4.94105406, -3.50945918, ...,  1.20452353,\n",
       "         3.5809826 ,  0.51992661],\n",
       "       [-4.94105406, 10.61708724,  6.69604535, ..., -2.7768132 ,\n",
       "        -3.20264287, -0.79965593],\n",
       "       [-3.50945918,  6.69604535,  7.92170028, ..., -3.7405506 ,\n",
       "        -0.21268301, -2.95536785],\n",
       "       ...,\n",
       "       [ 1.20452353, -2.7768132 , -3.7405506 , ...,  6.70215302,\n",
       "        -0.44118489, -0.08110758],\n",
       "       [ 3.5809826 , -3.20264287, -0.21268301, ..., -0.44118489,\n",
       "         6.93467274, -1.80715243],\n",
       "       [ 0.51992661, -0.79965593, -2.95536785, ..., -0.08110758,\n",
       "        -1.80715243,  4.35018343]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can check to see that $K$ is indeed positive semidefinite by computing a cholesky decompositionâ€”`cholesky()` from numpy's linalg can do that for us.  If it exists, $K$ must be at least PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.24533682e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [-2.20058480e+00,  2.40302180e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [-1.56299899e+00,  1.35518268e+00,  1.90845863e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       ...,\n",
       "       [ 5.36455609e-01, -6.64287416e-01, -1.04893038e+00, ...,\n",
       "         1.02275371e-06,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 1.59485320e+00,  1.27742011e-01,  1.10400988e+00, ...,\n",
       "        -2.46664055e-08,  1.03358295e-06,  0.00000000e+00],\n",
       "       [ 2.31558403e-01, -1.20719684e-01, -1.27319767e+00, ...,\n",
       "         2.83861383e-10, -5.83309832e-09,  1.01751738e-06]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  check PSD\n",
    "np.linalg.cholesky(k_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up and minimize the dual function given the constraints\n",
    "\n",
    "`cvxpy` is a package that allows the user to express a convex optimization problem in a readable form, converts it into a form that can be used to call a solver, solves, and then translates the result again into a readable form.  I can use `cvxpy`'s tools to encode the dual problem that I present above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = cp.Variable(shape=train_target.shape)\n",
    "\n",
    "beta = cp.multiply(alpha, train_target) # to simplify notation\n",
    "\n",
    "K = cp.Parameter(shape=k_value.shape, PSD=True, value=k_value)\n",
    "\n",
    "# objective function\n",
    "obj = .5 * cp.quad_form(beta, K) - np.ones(alpha.shape).T @ alpha\n",
    "\n",
    "# constraints\n",
    "const = [np.array(train_target.T) @ alpha == 0,\n",
    "        -alpha <= np.zeros(alpha.shape),\n",
    "        alpha <= 10*np.ones(shape=alpha.shape)]\n",
    "prob = cp.Problem(cp.Minimize(obj), const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = prob.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recreate the hyperplane "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.multiply(train_target, alpha.value).T @ train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = (alpha.value > 1e-4).flatten()\n",
    "b = train_target[S] - train_df[S] @ w\n",
    "b = b[0]\n",
    "# b = np.mean(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define and test out the classifier\n",
    "given the tuned parameters for the hyperplane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(x):\n",
    "    result = w @ x + b\n",
    "    return np.sign(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "incorrect = 0\n",
    "validation_set = credit_df_norm.drop(target, axis=1)\n",
    "predictions = []\n",
    "for i, x in validation_set.iterrows():\n",
    "    my_svm = classify(x)\n",
    "    if my_svm==credit_df_norm[target].replace(0, -1)[i]: correct +=1\n",
    "    else: incorrect +=1\n",
    "    predictions.append(my_svm)\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fraction correct: 0.61\n"
     ]
    }
   ],
   "source": [
    "print(f\"fraction correct: {correct/(correct + incorrect)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is the fraction of points for which my SVM correctly predicted the target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to sklearn_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_svm = svm.SVC(C = 10, kernel='linear')\n",
    "sklearn_svm.fit(train_df, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6075"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_svm.score(credit_df_norm.drop(target, axis=1), credit_df_norm[target])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
